# Computación Bayesiana

## Ejemplo Capítulo 5, Albert.

Librerías:

```{r}
library(tidyverse)
library(LearnBayes)
```

Datos de número de casos de cáncer de estómago en hombres en ciudades de Missouri, USA:

```{r}
data("cancermortality")
```

Gráfico de contorno de la logverosimilitud del modelo beta-binomial, ante la posible presencia de sobredispersión:

```{r}
mycontour(betabinexch0,c(.0001,.003,1,20000),cancermortality,
          xlab="eta",ylab="K")
```

en donde se evidencia una alta asimetría en ambos parámetros. Se reparametriza el modelo usando las transformaciones $\theta_1=\log\left(\frac{\eta}{1-\eta}\right)$, $\theta_2=\log K$ y se obtiene la siguiente logverisimilitud transformada:

```{r}
mycontour(betabinexch,c(-8,-4.5,3,16.5),cancermortality,
            xlab="logit eta",ylab="log K")
```

Aproximamos la log-densidad posterior usando el método de Laplace:

```{r}
fit <- laplace(betabinexch,c(-7,6),cancermortality)
```

con el gráfico de contorno de log-verosimilitud:

```{r}
npar=list(m=fit$mode,v=fit$var)
mycontour(lbinorm,c(-8,-4.5,3,16.5),npar,
              xlab="logit eta", ylab="log K")
```

Por lo tanto podemos sacar muestras posteriores de los parámetros transformados a través de una normal multivariada:

```{r}
set.seed(10)
library(mvtnorm)
mu_sigma_post <- rmvnorm(1000,mean = npar$m,sigma = npar$v)
```

y graficamos los puntos sobre el gráfico de contorno:

```{r}
mycontour(betabinexch,c(-8,-4.5,3,16.5),cancermortality,
            xlab="logit eta",ylab="log K")
points(mu_sigma_post,add=T)
```

A partir de esta muestra es posible obtener un intervalo de credibilidad exacto para cada parámetro usando el supuesto de normalidad:

```{r}
npar$m[1]+c(-1,1)*qnorm(0.975)*sqrt(npar$v[1,1])
npar$m[2]+c(-1,1)*qnorm(0.975)*sqrt(npar$v[2,2])
```

y por otro lado se puede calcular los intervalos de credibilidad directamente sobre la muestra:

```{r}
quantile(mu_sigma_post[,1],probs = c(0.025,0.975))
quantile(mu_sigma_post[,2],probs = c(0.025,0.975))
```

## Ejemplo sección 5.7, Albert


En un ejemplo anterior se había calculado la distribución posterior de la proporción de estudiantes que dormían más de 8 horas ($p$). Si queremos calcular los estimadores Monte Carlo de la probabilidad de que dos estudiantes duerman más de 8 horas ($p^2$) sería:

```{r}
p_posterior <- rbeta(1000,14.26,23.19)
hist(p_posterior)
est_posterior <- mean(p_posterior^2)
est_posterior
se_posterior <- sd(p_posterior^2)/sqrt(1000)
se_posterior
```

y a partir de esto se aproxima (usando el Teorema del Límite Central) un intervalo de credibilidad para $p^2$:

```{r}
est_posterior+c(-1,1)*qnorm(0.975)*se_posterior
```

## Continuación Ejemplo 5, Albert.

En este caso usaremos el algoritmo de rechazo como una forma de mejorar el proceso de muestreo obtenido a través del método de Laplace. Recuerden que para utilizar este algoritmo se necesita una distribución propuesta ($p(\theta)$) que aproxime la distribución posterior sin tomar en cuenta la constante de normalización. Si asumimos una distribución t de Student multivariada como propuesta, entonces podemos encontrar una cota superior para la diferencia entre las log-densidades a través de:

```{r}
betabinT=function(theta,datapar){
  data=datapar$data
  tpar=datapar$par
  d=betabinexch(theta,data)-dmt(theta,mean=c(tpar$m),
                                S=tpar$var,df=tpar$df,log=TRUE)
  return(d)
}

tpar=list(m=fit$mode,var=2*fit$var,df=4)
datapar=list(data=cancermortality,par=tpar)

start=c(-6.9,12.4)
fit1=laplace(betabinT,start,datapar)
```

de donde se observa que un posible valor para la diferencia sería:
```{r}
betabinT(fit1$mode,datapar)
```

y usamos este valor para obtener una muestra posterior de los parámetros usando el algoritmo de rechazo:

```{r}
set.seed(10)
theta=rejectsampling(betabinexch,tpar,-569.2813,10000,
                     cancermortality)
```

Noten que la proporción de aceptación en la muestra es:
```{r}
dim(theta)[1]/10000
```

y además podemos ver la mejora en la capacidad de la muestra posterior de estar localizada en regiones de alta probabilidad:

```{r}
mycontour(betabinexch,c(-8,-4.5,3,16.5),cancermortality,
          xlab="logit eta",ylab="log K")
points(theta[,1],theta[,2])
```

## Ejemplo sección 6.2, Albert.

En este ejemplo se ilustra una cadena discreta de Markov en donde una persona se mueve aleatoriamente en un conjunto de 6 estados solamente con un paso a la vez, y puede permanecer en el mismo estado de origen en cualquier momento. Si la probabilidad de permanecer en el mismo estado es igual a la probabilidad de avanzar o retroceder una posición, y asimismo las probabilidades de avanzar o retroceder son iguales, entonces la matriz de transición de la cadena de Markov asociada a este ejemplo es:

```{r}
P <- matrix(c(.5,.5,0,0,0,0,.25,.5,.25,0,0,0,0,.25,.5,.25,0,0,
           0,0,.25,.5,.25,0,0,0,0,.25,.5,.25,0,0,0,0,.5,.5),
         nrow=6,ncol=6,byrow=TRUE)
P
```

Si asumimos que la persona empieza en el estado 3 y simulamos el proceso de Markov a lo largo de 50000 pasos:

```{r}
s=array(0,c(50000,1))
s[1] <- 3
set.seed(10)

for (j in 2:50000){
  s[j]=sample(1:6,size=1,prob=P[s[j-1],])
}
```

Graficamente, los últimos 1000 pasos se pueden representar:
```{r}
plot(tail(s,1000),type='l')
```

Debido a que esta cadena de Markov es irreducible y aperiódica existe una distribución estacionaria que vamos a aproximar a través del cálculo de la frecuencia relativa en cada uno de los 6 estados:

```{r}
m=c(500,2000,8000,50000)

for (i in 1:4){
  print(table(s[1:m[i]])/m[i])
}
```

lo cual parece indicar de que hay convergencia a una frecuencia relativa estable por clase. La distribución estacionaria exacta se calcula a través del vector propio asociado al valor propio igual a 1 de la matriz $P^T$:

```{r}
eig_des <- eigen(t(P))
eig_des$values
dist_estacionaria <- eig_des$vectors[,1]
dist_estacionaria <- dist_estacionaria /sum(dist_estacionaria)
show(dist_estacionaria)
```

## Ejemplo sección 6.7, Albert.

Usando la siguiente tabla de datos agrupados, queremos hacer inferencia de la media $\mu$ y desviación estándar $\sigma$ de la altura en pulgadas de hombres universitarios:

```{r}
d=list(int.lo=c(-Inf,seq(66,74,by=2)),
         int.hi=c(seq(66,74,by=2), Inf),
         f=c(14,30,49,70,33,15))
show(d)
```

donde los vectores int.hi y int.lo indican los límites superiores e inferiores de los datos agrupados (en pulgadas). El vector f indica la frecuencia de individuos por intervalo. Asumiendo una distribución normal en la altura de los individuos, y tomando la transformación $\lambda=\log(\sigma)$, la distribución posterior se implementaría:

```{r}
groupeddatapost=function(theta,data)
{
  dj = function(f, int.lo, int.hi, mu, sigma)
    f * log(pnorm(int.hi, mu, sigma) -
              pnorm(int.lo, mu, sigma))
  mu = theta[1]
  sigma = exp(theta[2])
  sum(dj(data$f, data$int.lo, data$int.hi, mu, sigma))
}
```

Primero aproximamos la posterior a través de una normal multivariada con el método de Laplace. Para ello simulamos observaciones por nivel en la tabla agrupada, asumiendo que las alturas son constantes por grupo:

```{r}
y <- c(rep(65,14),rep(67,30),rep(69,49),rep(71,70),rep(73,33),
    rep(75,15))

mean(y)
log(sd(y))
```

y a partir de lo anterior tomamos como punto inicial (70,1) en el algoritmo de Laplace:

```{r}
start <- c(70,1)
fit <- laplace(groupeddatapost,start,d)
```

y recuerden que se pueden generar muestras posteriores de $(\mu,\lambda)$ con este método:

```{r}
mu_lambda_post <- rmvnorm(5000,mean = fit$m,sigma = fit$v)
```

Usando el cálculo anterior, tomamos como propuesta en el algoritmo de Metropolis-Hastings con caminata aleatoria una normal multivariada con escala = 2 y la misma matriz de varianza-covarianza anterior:

```{r}
proposal <- list(var=fit$var,scale=2)
bayesfit <- rwmetrop(groupeddatapost,proposal,start,10000,d)
```

La muestra posterior resultante tuvo una tasa de aceptación de:

```{r}
bayesfit$accept
```

El paquete coda nos permite graficar los traceplots de cada muestra posterior. Antes de eso, definimos un objeto mcmc de coda:

```{r}
library(coda)
dimnames(bayesfit$par)[[2]]=c("mu","log sigma")
mcmc_ej <- mcmc(bayesfit$par[-c(1:2000),]) 
```

```{r}
library(coda)
library(lattice)
xyplot(mcmc_ej,col="black")
```

Note que usamos un periodo de quemado (burn-in) del 2000 muestras. También podemos incluir un gráfico de autocorrelación:

```{r,fig.width=8,fig.height=6}
par(mfrow=c(2,1))
autocorr.plot(mcmc_ej,auto.layout=FALSE)
```
y por otro lado uno puede tener un resumen de las muestras posteriores vía MCMC:

```{r}
summary(mcmc_ej)
```

